{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f26ddd3",
   "metadata": {},
   "source": [
    "\n",
    "# Lab 9: Build a Log Aggregator\n",
    "## Alexander Land and Maggie Tapia\n",
    "\n",
    "In this lab, you will create your own log generator, build a command-line utility that scans log files, summarizes their contents, and provides insight into system behavior. Data structures to track log message levels such as `INFO`, `WARNING`, `ERROR`, and `CRITICAL`.\n",
    "\n",
    "This lab reinforces:\n",
    "- File I/O\n",
    "- Pattern recognition (regex)\n",
    "- Dictionaries and counters\n",
    "- Functions and modularity\n",
    "- Optional: CLI arguments, logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5ee8a",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Create Log files (20%)\n",
    "Using the the following example log format below create a **python file** that will log errors In a structured tree format \n",
    "\n",
    "You will find examples in the folder called Logs that you can use to build your program.\n",
    "\n",
    "Remember set of logs should have a varied levels of log entries (`INFO`, `WARNING`, `ERROR`, `CRITICAL`) and tailored message types for different service components.\n",
    "You must create 5 structured logs here are some examples:\n",
    "\n",
    "    sqldb\n",
    "    ui\n",
    "    frontend.js\n",
    "    backend.js\n",
    "    frontend.flask\n",
    "    backend.flask\n",
    "\n",
    "You may use chat GPT to create sample outputs NOT THE LOGS. IE:\n",
    "\n",
    "    System failure\n",
    "    Database corruption\n",
    "    Disk failure detected\n",
    "    Database corruption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ba30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your python file here \n",
    "# don't forget to upload it with your submission\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "import random\n",
    "\n",
    "logging.basicConfig(\n",
    "    level= logging.DEBUG, \n",
    "    format= '%(asctime)s | %(name)s |%(levelname)s | %(message)s' # idk if asctime works\n",
    "    )\n",
    "\n",
    "formatter = logging.Formatter(\n",
    "    fmt=('%(asctime)s | %(name)s |%(levelname)s | %(message)s' \n",
    "    )\n",
    ")\n",
    "\n",
    "# loggers\n",
    "sqldb_Log = logging.getLogger(\"sql_logger\")\n",
    "sqldb_Log.setLevel(logging.INFO)\n",
    "\n",
    "frontend_Log = logging.getLogger(\"frontend\")\n",
    "frontend_Log.setLevel(logging.INFO)\n",
    "\n",
    "frontend_js_Log = logging.getLogger(\"frontend.js\")\n",
    "frontend_js_Log.setLevel(logging.INFO)\n",
    "\n",
    "frontend_flask_Log = logging.getLogger(\"frontend.flask\")\n",
    "frontend_flask_Log.setLevel(logging.INFO)\n",
    "\n",
    "frontend_flask_layer_Log = logging.getLogger(\"frontend.flask.layer\")\n",
    "frontend_flask_layer_Log.setLevel(logging.INFO)\n",
    "\n",
    "# handlers\n",
    "\n",
    "sql_handler = logging.handlers.TimedRotatingFileHandler(\n",
    "    filename= \"sql.log\",\n",
    "    when= \"D\",\n",
    "    backupCount= 1\n",
    ")\n",
    "\n",
    "frontend_handler = logging.handlers.TimedRotatingFileHandler(\n",
    "    filename= \"frontend.log\",\n",
    "    when= \"D\",\n",
    "    backupCount= 1\n",
    ")\n",
    "\n",
    "frontend_js_handler = logging.handlers.TimedRotatingFileHandler(\n",
    "    filename= \"frontend_js.log\",\n",
    "    when= \"D\",\n",
    "    backupCount= 1\n",
    ")\n",
    "\n",
    "frontend_flask_handler = logging.handlers.TimedRotatingFileHandler(\n",
    "    filename= \"frontend_flask.log\",\n",
    "    when= \"D\",\n",
    "    backupCount= 1\n",
    ")\n",
    "\n",
    "frontend_flask_layer_handler = logging.handlers.TimedRotatingFileHandler(\n",
    "    filename= \"frontend_flask_layer.log\",\n",
    "    when= \"D\",\n",
    "    backupCount= 1\n",
    ")\n",
    "\n",
    "# add the handlers to the loggers\n",
    "sqldb_Log.addHandler(sql_handler)\n",
    "frontend_Log.addHandler(frontend_handler)\n",
    "frontend_js_Log.addHandler(frontend_js_handler)\n",
    "frontend_flask_Log.addHandler(frontend_flask_handler)\n",
    "frontend_flask_layer_Log.addHandler(frontend_flask_layer_handler)\n",
    "\n",
    "# add formatter to handlers\n",
    "sql_handler.setFormatter(formatter)\n",
    "frontend_handler.setFormatter(formatter)\n",
    "frontend_js_handler.setFormatter(formatter)\n",
    "frontend_flask_handler.setFormatter(formatter)\n",
    "frontend_flask_layer_handler.setFormatter(formatter)\n",
    "\n",
    "\n",
    "\n",
    "# make the log messages\n",
    "\n",
    "#I asked chat for comedic error messages\n",
    "list_of_potential_errors = [\n",
    "    \"Something went wrong. Probably your fault.\",\n",
    "    \"Uncaught exception in user behavior.\",\n",
    "    \"The system has given up. Please proceed manually.\",\n",
    "    \"Confidence.exe unexpectedly closed.\",\n",
    "    \"Sanity module missing.\",\n",
    "    \"Task completion not found.\",\n",
    "    \"You\\'ve been idle for 3 hours. We assumed you were crying.\",\n",
    "    \"Tried to operate before coffee. Not permitted.\",\n",
    "    \"Thought process overflowed into dream state.\",\n",
    "    \"Recollection of why you walked into the room not available.\",\n",
    "    \"No idea what you\\'re doing.\",\n",
    "    \"Too many browser tabs. One is now sentient.\",\n",
    "    \"Too many recursive thoughts.\",\n",
    "    \"You can\\'t just do that because you feel like it.\",\n",
    "    \"Cannot install new personality. Files corrupted.\",\n",
    "    \"This seemed like a good idea 3 hours ago.\",\n",
    "    \"You argued with someone on the internet.\",\n",
    "    \"Connection lost to reality.\",\n",
    "    \"Your ambition has been archived or deleted.\",\n",
    "    \"Yes, everyone saw that. No, you can\\'t undo it.\"\n",
    "]\n",
    "\n",
    "def random_error_level(logger, message):\n",
    "    \"\"\"takes in the logger and message and gives it a random error level\"\"\"\n",
    "    match random.randint(0,4):\n",
    "        case 0: logger.critical(message)\n",
    "        case 1: logger.error(message)\n",
    "        case 2: logger.warning(message)\n",
    "        case 3: logger.info(message)\n",
    "        case 4: logger.debug(message)\n",
    "\n",
    "\n",
    "# start of main code\n",
    "for i in range(50):\n",
    "    random_error_message = list_of_potential_errors[random.randint(0, len(list_of_potential_errors)-1)]\n",
    "    \n",
    "    match random.randint(0,4):\n",
    "        # assigns puts random logger into the random error level to give a randomized error source, level, and message.\n",
    "        case 0: random_error_level(sqldb_Log, random_error_message)\n",
    "        case 1: random_error_level(frontend_Log, random_error_message)\n",
    "        case 2: random_error_level(frontend_js_Log, random_error_message)\n",
    "        case 3: random_error_level(frontend_flask_Log, random_error_message)\n",
    "        case 4: random_error_level(frontend_flask_layer_Log, random_error_message)\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5255ab",
   "metadata": {},
   "source": [
    "\n",
    "### Example Log Format\n",
    "\n",
    "You will work with logs that follow this simplified structure:\n",
    "\n",
    "```\n",
    "2025-04-11 23:20:36,913 | my_app | INFO | Request completed\n",
    "2025-04-11 23:20:36,914 | my_app.utils | ERROR | Unhandled exception\n",
    "2025-04-11 23:20:36,914 | my_app.utils.db | CRITICAL | Disk failure detected\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659dfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af5f6e84",
   "metadata": {},
   "source": [
    "## Part 2: Logging the Log File (40%)\n",
    "    New File\n",
    "### Part 2a: Read the Log File (see lab 7) (10%)\n",
    "\n",
    "\n",
    "Write a function to read the contents of a log file into a list of lines. Handle file errors gracefully.\n",
    "\n",
    "### Part 2b: Parse Log Lines (see code below if you get stuck) (10%)\n",
    "\n",
    "Use a regular expression to extract:\n",
    "- Timestamp\n",
    "- Log name\n",
    "- Log level\n",
    "- Message\n",
    "\n",
    "### Part 2c: Count Log Levels (20%)\n",
    "\n",
    "Create a function to count how many times each log level appears. Store the results in a dictionary. Then output it as a Json File\n",
    "You may pick your own format but here is an example. \n",
    "```python\n",
    "{\n",
    "    \"INFO\": \n",
    "    {\n",
    "        \"Request completed\": 42, \n",
    "        \"Heartbeat OK\": 7\n",
    "    }\n",
    "\n",
    "    \"WARNING\":\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc631f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your python file here don't for get to upload it with your submission\n",
    "import re\n",
    "import json\n",
    "\n",
    "def readlog(logfilename):\n",
    "    \"\"\"reads the log file, parses it, and returns a dict with the number of occurances of each error message at each error level\n",
    "    It also can return a list of each log if slightly modified.\"\"\"\n",
    "    # list_of_logs = [] #unused list that also holds each log\n",
    "    dict_of_log = {}\n",
    "    # regex pattern that puls out, the time, log_name, log_level, and message. Chat made the regex but I made everything else based on NGCP code I made\n",
    "    pattern = r'^(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}) \\| ([^|]+) \\|([A-Z]+) \\| (.+)$' \n",
    "\n",
    "\n",
    "    try:\n",
    "        with open(logfilename) as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            for line in lines: \n",
    "                match = re.search(pattern, line) #checks line by line for the regex pattern\n",
    "\n",
    "                if match:\n",
    "                    timestamp, log_name, log_level, message = match.groups() # if a line is match it'll section each section of the data into its respective value\n",
    "                    # list_of_logs.append({\"timestamp\": timestamp, \"log name\": log_name, \"log level\":log_level, \"message\": message})  #unused list it could output\n",
    "\n",
    "                    if log_level not in dict_of_log: #adds log level if its not in the dictionary\n",
    "                        dict_of_log[log_level] = {}\n",
    "                    \n",
    "                    if message not in dict_of_log[log_level]: # adds the message if its not in the dictionary at that log level\n",
    "                        dict_of_log[log_level][message] = 0\n",
    "\n",
    "                    dict_of_log[log_level][message] += 1 #since the log level and message are in the dict with the above part, this will increment it every time that message shows up\n",
    "    except Exception as e: # I learned this neat way of handling erros so it'll print the error if something happens when opening the file\n",
    "        print(f\"error {e}\")\n",
    "\n",
    "    return dict_of_log\n",
    "\n",
    "\n",
    "def save_to_json(dictionary,filename): \n",
    "    \"\"\"takes dict and dumps it in json file\"\"\"\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(dictionary, json_file, indent=4)\n",
    "\n",
    "def loader(filename): \n",
    "    \"\"\"opens json file, prints it readibly, then returns it as a dict\"\"\"\n",
    "    with open(filename) as file:\n",
    "        data = json.load(file)\n",
    "        neater_data = json.dumps(data, indent=2)\n",
    "        print(neater_data)\n",
    "    return data\n",
    "\n",
    "\n",
    "save_to_json(readlog(\"frontend.log\"), \"frontend.json\")\n",
    "loader(\"frontend.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your python file here \n",
    "# don't forget to upload it with your submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045c30f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Generate Summary Report (40%)\n",
    "    New File\n",
    "### Step 3a (20%):\n",
    " Develop a function that continuously monitors your JSON file(s) and will print a real-time summary of log activity. It should keep count of the messages grouped by log level (INFO, WARNING, ERROR, CRITICAL) and display only the critical messages. (I.e. If new data comes in the summary will change and a new critical message will be printed)\n",
    " - note: do not reprocess the entire file on each update.  \n",
    "\n",
    "### Step 3a: Use a Matplotlib (Lecture 10) (20%)\n",
    "Develop a function that continuously monitors your JSON file(s) and will graph in real-time a bar or pie plot of each of the errors.  (a graph for each log level). \n",
    "- The graph should show the distribution of log messages by level  (INFO, WARNING, ERROR, CRITICAL)  \n",
    "\n",
    "\n",
    "### Critical notes:\n",
    "- Your code mus use Daemon Threads (Lecture 14)\n",
    "- 3a and 3b do not need to run at the same time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your python file here \n",
    "# don't forget to upload it with your submission\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "#loader from log reader\n",
    "def loader(filename):\n",
    "    \"\"\"opens json file, prints it readibly, then returns it as a dict\"\"\"\n",
    "    with open(filename) as file:\n",
    "        data = json.load(file)\n",
    "        neater_data = json.dumps(data, indent=2)\n",
    "        print(neater_data)\n",
    "    return data\n",
    "\n",
    "def log_json_to_graph(filename):\n",
    "    \"\"\"opens json log file and graphs the number of errors at each error level\"\"\"\n",
    "    categories = []\n",
    "    values = []\n",
    "    data = loader(filename)\n",
    "\n",
    "    # makes a dictionary that is indexed at error level and keeps track of the number of errors at that level\n",
    "    count_of_errors = {}\n",
    "    for log_level in data: \n",
    "        count_of_errors[log_level] = 0  # for each log level make a key with a value of 0\n",
    "        for error in data[log_level]:  \n",
    "            count_of_errors[log_level] += data[log_level][error]    \n",
    "            #for each error message it'll check how many times it occurs and increment that number to whichever log level its currently iterating through\n",
    "\n",
    "    for error_level in count_of_errors: # puts the keys from counter_of_errors dictionary as catagores and values as values\n",
    "        categories.append(error_level)\n",
    "        values.append(count_of_errors[error_level])\n",
    "\n",
    "    # plots a bar graph for the data\n",
    "    plt.bar(categories, values)\n",
    "    plt.title(f'Number of errors in: {filename}')\n",
    "    plt.xlabel('Error Levels')\n",
    "    plt.ylabel('# of errors')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "log_json_to_graph(\"frontend.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26eb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a sample regex that parses a log file and extracts relevant information. \n",
    "# you will need to modify it. Review Lecture 11\n",
    "import re\n",
    "\n",
    "def parse_log_line(line):\n",
    "    pattern = r\"^(.*?)\\s\\|\\s(\\w+)\\s\\|\\s(\\w+)\\s\\|\\s(.*)$\"\n",
    "    match = re.match(pattern, line)\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
